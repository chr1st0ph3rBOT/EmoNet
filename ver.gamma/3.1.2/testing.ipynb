{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f53162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda, dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# 환경\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"  # 토치 컴파일 비활성\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\"); torch_dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\"); torch_dtype = torch.float32\n",
    "\n",
    "print(f\"Device: {device}, dtype: {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380daabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\gptoss\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You have loaded an FP4 model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model. To remove this warning, pass device_map = 'cuda'. \n",
      "Fetching 41 files: 100%|██████████| 41/41 [00:00<?, ?it/s]\n",
      "Fetching 41 files: 100%|██████████| 41/41 [00:00<?, ?it/s]?it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 로드\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "MODEL_ID = \"openai/gpt-oss-20b\"\n",
    "REVISION = None\n",
    "\n",
    "_tok = AutoTokenizer.from_pretrained(MODEL_ID, revision=REVISION, trust_remote_code=True, use_fast=True)\n",
    "if _tok.pad_token is None:\n",
    "    _tok.pad_token = _tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=REVISION,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=False,\n",
    "    device_map=None,\n",
    ")\n",
    "model.to(device); model.eval()\n",
    "\n",
    "_gen = pipeline(\"text-generation\", model=model, tokenizer=_tok,\n",
    "                device=0 if device.type==\"cuda\" else -1)\n",
    "\n",
    "print(\"Model & tokenizer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4e831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda, dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 41 files: 100%|██████████| 41/41 [00:00<?, ?it/s]\n",
      "Fetching 41 files: 100%|██████████| 41/41 [00:00<?, ?it/s]?it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.34s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Of the allocated memory 37.46 GiB is allocated by PyTorch, and 383.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     51\u001b[39m     _tok.pad_token = _tok.eos_token\n\u001b[32m     53\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     54\u001b[39m     MODEL_ID,\n\u001b[32m     55\u001b[39m     revision=REVISION,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     device_map=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     60\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m model.to(device); model.eval()\n\u001b[32m     63\u001b[39m _gen = pipeline(\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m     model=model,\n\u001b[32m     66\u001b[39m     tokenizer=_tok,\n\u001b[32m     67\u001b[39m     device=\u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device.type == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m,\n\u001b[32m     68\u001b[39m )\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel & tokenizer ready.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\anaconda3\\envs\\gptoss\\Lib\\site-packages\\transformers\\modeling_utils.py:4459\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4455\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4456\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4457\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4458\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4459\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\anaconda3\\envs\\gptoss\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\anaconda3\\envs\\gptoss\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\anaconda3\\envs\\gptoss\\Lib\\site-packages\\torch\\nn\\modules\\module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\anaconda3\\envs\\gptoss\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Of the allocated memory 37.46 GiB is allocated by PyTorch, and 383.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "NTL(Neurotransmitter Tendency List) 벡터라이저 — GPT-OSS 20B 기반\n",
    "입력 문자열 -> [d, s, n, m]([도파민, 세로토닌, 노르에피네프린, 멜라토닌])\n",
    "\n",
    "요구사항:\n",
    "- LLM 학습(파인튜닝) 없이, 프롬프트만으로 동작\n",
    "- 출력 형식 엄수: 소수(0.0~1.0), 길이 4\n",
    "- GPT-OSS가 신경전달물질 개념을 이해할 수 있도록, 프롬프트에 정의 포함\n",
    "- 파서/검증, 배치 지원 + 이유(reason) 라인 추가\n",
    "\"\"\"\n",
    "\n",
    "# =============================\n",
    "# 환경\n",
    "# =============================\n",
    "import os, re, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"  # 토치 컴파일 비활성\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\"); torch_dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\"); torch_dtype = torch.float32\n",
    "\n",
    "print(f\"Device: {device}, dtype: {torch_dtype}\")\n",
    "\n",
    "# =============================\n",
    "# 모델 및 토크나이저 로드\n",
    "# =============================\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "MODEL_ID = \"openai/gpt-oss-20b\"\n",
    "REVISION = None\n",
    "\n",
    "_tok = AutoTokenizer.from_pretrained(MODEL_ID, revision=REVISION, trust_remote_code=True, use_fast=True)\n",
    "if _tok.pad_token is None:\n",
    "    _tok.pad_token = _tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    revision=REVISION,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=False,\n",
    "    device_map=None,\n",
    ")\n",
    "model.to(device); model.eval()\n",
    "\n",
    "_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=_tok,\n",
    "    device=0 if device.type == \"cuda\" else -1,\n",
    ")\n",
    "\n",
    "print(\"Model & tokenizer ready.\")\n",
    "\n",
    "# =============================\n",
    "# 프롬프트 템플릿\n",
    "# =============================\n",
    "SYSTEM_PRIMER = (\n",
    "    \"너는 신경과학 보조자다. 아래 지시를 정확히 따른다.\\n\"\n",
    "    \"목표: 어떤 입력 문장을 신경전달물질 경향 벡터 NTL로 변환한다.\\n\"\n",
    "    \"NTL 포맷은 반드시 다음과 같다: \"\n",
    "    \"[d, s, n, m]([도파민, 세로토닌, 노르에피네프린, 멜라토닌])\\n\"\n",
    "    \"여기서 d,s,n,m은 0.0~1.0 범위의 소수(최대 소수점 3자리)다.\\n\"\n",
    "    \"추가로 두 번째 줄에 반드시 '이유: '로 시작하는 간단한 설명을 한 문장으로 제공한다.\\n\\n\"\n",
    "    \"정의 요약:\\n\"\n",
    "    \"- 도파민(dopamine): 동기/보상/추진력·목표지향. 기대/새로움 추구.\\n\"\n",
    "    \"- 세로토닌(serotonin): 안정/만족/평온·사회적 유대. 충동 억제·기분 균형.\\n\"\n",
    "    \"- 노르에피네프린(norepinephrine): 각성/집중/경계·에너지·스트레스 반응.\\n\"\n",
    "    \"- 멜라토닌(melatonin): 수면/암시성·생체리듬·야간 안정.\\n\\n\"\n",
    "    \"지침:\\n\"\n",
    "    \"1) 입력의 정서, 의도, 맥락을 읽고 네 신경전달물질 경향을 정규화(0~1) 점수로 매긴다.\\n\"\n",
    "    \"2) 합은 1이 될 필요 없음. 네 축은 독립적이다.\\n\"\n",
    "    \"3) 출력은 정확히 두 줄만: 첫 줄은 NTL 포맷, 둘째 줄은 '이유: ' 설명. 다른 텍스트 금지.\\n\"\n",
    "    \"예시:\\n\"\n",
    "    \"입력: '밤새 코딩해서 데드라인 맞추고 싶어.'\\n\"\n",
    "    \"출력: [0.840, 0.210, 0.690, 0.050]([도파민, 세로토닌, 노르에피네프린, 멜라토닌])\\n\"\n",
    "    \"이유: 목표 달성 욕구와 각성/긴장이 높고, 안정/수면은 낮다.\\n\"\n",
    ")\n",
    "\n",
    "USER_WRAPPER = (\n",
    "    \"다음 문장을 NTL로 변환하라. 형식 엄수: 첫 줄은 [d, s, n, m]([도파민, 세로토닌, 노르에피네프린, 멜라토닌]), \"\n",
    "    \"둘째 줄은 '이유: ' 한 문장.\\n\"\n",
    "    \"문장: \"\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 파싱/검증 유틸\n",
    "# =============================\n",
    "_float = r\"(?:0(?:\\.\\d{1,3})?|1(?:\\.0{1,3})?|0?\\.\\d{1,3}|0|1)\"\n",
    "STRICT_RE = re.compile(\n",
    "    rf\"\\[\\s*({_float})\\s*,\\s*({_float})\\s*,\\s*({_float})\\s*,\\s*({_float})\\s*\\]\\(\\[\\s*도파민\\s*,\\s*세로토닌\\s*,\\s*노르에피네프린\\s*,\\s*멜라토닌\\s*\\]\\)\"\n",
    ")\n",
    "\n",
    "LABELS = [\"도파민\", \"세로토닌\", \"노르에피네프린\", \"멜라토닌\"]\n",
    "\n",
    "def _clip01(x: float) -> float:\n",
    "    return float(min(1.0, max(0.0, x)))\n",
    "\n",
    "def parse_ntl(text: str):\n",
    "    \"\"\"\n",
    "    모델 출력에서 NTL 벡터와 이유를 파싱하고 검증한다.\n",
    "    반환: (vector: List[float], labels: List[str], reason: str)\n",
    "    \"\"\"\n",
    "    raw = text.strip()\n",
    "    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]\n",
    "\n",
    "    # 1) 라벨 포함 정규식으로 벡터 라인 우선 탐색\n",
    "    vec = None\n",
    "    for ln in lines:\n",
    "        m = STRICT_RE.search(ln)\n",
    "        if m:\n",
    "            vec = [_clip01(float(g)) for g in m.groups()]\n",
    "            break\n",
    "\n",
    "    # 2) 형식이 어긋난 경우: 첫 []에서 숫자 4개만 추출\n",
    "    if vec is None:\n",
    "        for ln in lines:\n",
    "            if \"[\" in ln and \"]\" in ln:\n",
    "                m2 = re.search(r\"\\[([^\\]]+)\\]\", ln)\n",
    "                if not m2:\n",
    "                    continue\n",
    "                try:\n",
    "                    nums = [float(x) for x in m2.group(1).split(\",\")]\n",
    "                    nums = nums[:4]\n",
    "                    if len(nums) == 4:\n",
    "                        vec = [_clip01(float(x)) for x in nums]\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if vec is None:\n",
    "            raise ValueError(f\"출력 파싱 실패: {text}\\n(raw=\\n{raw}\\n)\")\n",
    "\n",
    "    # 이유 라인 추출\n",
    "    reason = \"\"\n",
    "    for ln in lines:\n",
    "        if ln.lower().startswith(\"이유:\") or ln.lower().startswith(\"reason:\"):\n",
    "            reason = ln.split(\":\", 1)[1].strip()\n",
    "            break\n",
    "\n",
    "    return vec, LABELS, reason\n",
    "\n",
    "# =============================\n",
    "# 생성 파이프 설정\n",
    "# =============================\n",
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    max_new_tokens: int = 48\n",
    "    temperature: float = 0.0  # 결정적 생성\n",
    "    top_p: float = 0.95\n",
    "    repetition_penalty: float = 1.05\n",
    "    stop: Tuple[str, ...] = ()\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, generator_pipeline, system_primer: str = SYSTEM_PRIMER):\n",
    "        self.gen = generator_pipeline\n",
    "        self.system = system_primer\n",
    "\n",
    "    def _format_prompt(self, text: str) -> str:\n",
    "        return self.system + \"\\n\\n\" + USER_WRAPPER + text.strip()\n",
    "\n",
    "    def encode_ntl(self, text: str, config: GenerationConfig = GenerationConfig()) -> Dict[str, Any]:\n",
    "        prompt = self._format_prompt(text)\n",
    "        out_full = self.gen(\n",
    "            prompt,\n",
    "            max_new_tokens=config.max_new_tokens,\n",
    "            temperature=config.temperature,\n",
    "            do_sample=bool(config.temperature and config.temperature > 0.0),\n",
    "            top_p=config.top_p,\n",
    "            repetition_penalty=config.repetition_penalty,\n",
    "            eos_token_id=_tok.eos_token_id,\n",
    "            pad_token_id=_tok.pad_token_id,\n",
    "        )[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "        try:\n",
    "            vec, labels, reason = parse_ntl(out_full)\n",
    "        except Exception:\n",
    "            # 최후 수단 복구: 숫자 4개만 긁어서 벡터 구성\n",
    "            nums = re.findall(r\"\\d*\\.\\d+|\\d+\", out_full)\n",
    "            if len(nums) >= 4:\n",
    "                vec = [_clip01(float(x)) for x in nums[:4]]\n",
    "                labels = LABELS\n",
    "                reason = \"\"\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        return {\n",
    "            \"input\": text,\n",
    "            \"vector\": vec,\n",
    "            \"labels\": labels,\n",
    "            \"reason\": reason,\n",
    "            \"formatted\": f\"[{vec[0]:.3f}, {vec[1]:.3f}, {vec[2]:.3f}, {vec[3]:.3f}]([도파민, 세로토닌, 노르에피네프린, 멜라토닌])\",\n",
    "            \"raw\": out_full.strip(),\n",
    "        }\n",
    "\n",
    "    def batch_encode_ntl(self, texts: List[str], config: GenerationConfig = GenerationConfig()) -> List[Dict[str, Any]]:\n",
    "        return [self.encode_ntl(t, config) for t in texts]\n",
    "\n",
    "# =============================\n",
    "# 사용 예시 & 간단 테스트\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    llm = LLM(_gen)\n",
    "\n",
    "    # 샘플 문장 4개\n",
    "    samples = [\n",
    "        \"새로 시작한 프로젝트에 너무 설레고 집중이 잘 돼. 밤을 새워서라도 끝내고 싶어.\",\n",
    "        \"요즘은 마음이 잔잔하고 친구들이랑 함께 있으면 안정돼.\",\n",
    "        \"마감이 가까워져서 긴장되고 머리가 더 예민해진 느낌이야.\",\n",
    "        \"불 꺼진 방에서 금방 잠들 준비가 됐어.\"\n",
    "    ]\n",
    "\n",
    "    # 1) 개별 실행 출력\n",
    "    print(\"\\n=== SAMPLE RUN ===\")\n",
    "    for s in samples:\n",
    "        res = llm.encode_ntl(s)\n",
    "        print(json.dumps({\n",
    "            \"input\": res[\"input\"],\n",
    "            \"formatted\": res[\"formatted\"],\n",
    "            \"reason\": res.get(\"reason\", \"\")\n",
    "        }, ensure_ascii=False))\n",
    "\n",
    "    # 2) 파서 회복력 테스트 (LLM 호출 없이 파서만)\n",
    "    print(\"\\n=== PARSER TESTS ===\")\n",
    "    noisy = \"\"\"\n",
    "    이런저런 서론... 무시해도 됨\n",
    "    [0.8, 0.5, 0.12, 0.456]([도파민, 세로토닌, 노르에피네프린, 멜라토닌])\n",
    "    이유: 테스트 문장 — 동기/보상 및 각성 경향.\n",
    "    꼬릿말...\n",
    "    \"\"\"\n",
    "    v, labels, r = parse_ntl(noisy)\n",
    "    assert len(v) == 4 and all(0.0 <= x <= 1.0 for x in v)\n",
    "    assert labels == [\"도파민\", \"세로토닌\", \"노르에피네프린\", \"멜라토닌\"]\n",
    "    assert isinstance(r, str)\n",
    "\n",
    "    unlabeled = \"\"\"\n",
    "    [0.3, 0.4, 0.2, 0.1]\n",
    "    이유: 라벨이 없어도 복구.\n",
    "    \"\"\"\n",
    "    v2, labels2, r2 = parse_ntl(unlabeled)\n",
    "    assert len(v2) == 4 and all(0.0 <= x <= 1.0 for x in v2)\n",
    "    assert labels2 == labels\n",
    "\n",
    "    print(\"[TEST PASS] 기본 파서/출력 흐름 OK\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptoss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
