# ver.theta 상세 설계

이 문서는 논문 초안 수준의 설명으로, 코드를 직접 보지 않아도 ver.theta의 동작 개념을 이해할 수 있도록 구성한다.

## 개요
ver.theta는 감정 상태 요약 벡터(`NTL`)와 내부 상태(`K`), 그리고 임시 기억 스택(`IPT`)을 통해 학습 신호를 조절하는 신경망 설계를 목표로 한다. 핵심 아이디어는 **상태 요약 → 가중치 변화량(ΔW) 조절**의 흐름을 명확히 정의하는 것이다.

## 핵심 상태 변수
- `NTL=[D,S,NE,M]`: 네트워크 전체 상태를 요약하는 4차원 벡터다. 각 요소는 특정 정서·활성 패턴을 대변하며, 학습 및 가중치 변동의 조절 신호로 사용된다.
- `K`(고정 범위): 입력 뉴런들의 상태와 가중치로부터 계산되는 내부 값이다. 계산 결과가 무제한으로 커지는 것을 방지하기 위해, 지정된 고정 범위로 클리핑된다.
- `IPT`(메모리 스택): 최근 입력/상태를 임시로 저장하는 스택 구조다. 장·단기 기억을 분리하기 위한 기초로 사용되며, 규칙은 실험적으로 조정 가능하다.

## K 업데이트 규칙
1. 입력 뉴런의 K와 해당 가중치의 선형 결합을 계산한다.
2. 계산된 값을 사전에 정의된 고정 범위로 클리핑한다.
3. 클리핑된 결과를 새로운 K로 갱신한다.

이 과정은 K가 입력 변화에 반응하되 폭주하지 않도록 제약한다.

## ΔW 정의
`ΔW`는 연결된 뉴런 수 또는 연결성의 크기에 의해 결정되는 값이다. 즉, 더 큰 연결성은 더 큰 변화량 가능성을 의미한다. 단, 실제 변화량은 `NTL`에 의해 조절된다.

## ΔW 조절 원칙
`NTL`은 `ΔW`의 변화율과 증감 폭을 제어한다. 이는 **감정 상태 요약이 학습 강도를 직접 조절한다**는 설계 결정이다. 따라서 동일한 입력이라도 `NTL`이 다르면 가중치 업데이트의 강도와 방향성이 달라질 수 있다.

## NTL 추출 방식
`NTL`은 기본적으로 LLM을 통해 추출한다. ML 기반 추정은 정확도가 낮은 fallback으로만 사용하며, 시스템의 기본 경로는 LLM 기반 추출에 있다.

## 메모리 규칙
`IPT`는 현재 임시 규칙에 기반해 동작한다. 이는 초기 설계 단계에서의 가변 요소로, 향후 실험 결과에 따라 변경 가능함을 명시한다.

---

# 구현 상세(코드 기반)
아래 내용은 `ver.theta/main.py` 구현을 바탕으로 정리한 기술 문서다.

## 실행 흐름 요약
1. GUI에서 텍스트와 NTL 값을 입력한다.
2. `Inject`로 입력을 네트워크에 투입한다.
3. `Tick`으로 네트워크를 한 단계 갱신한다.
4. 네트워크 출력(최근 NTL/K/메모리)과 통계가 로그 및 UI에 기록된다.

## 주요 상수 및 스케일링 규칙
- `K_MIN/K_MAX`: K 값 클램프 범위(0.0~2.0).
- `CONNECTION_DELTA_SCALE`: 연결 변화량(ΔW)의 최대 절대값(3).
- `MEMORY_LIMIT`: 뉴런 메모리 큐의 최대 길이(50).
- `K_REMEM`, `W_REMEM`: 기억 저장의 임계 기준(K, W).
- `K_LEAK`, `K_SPIKE_BONUS_ALPHA`: K 누설 및 NE 보너스 계수.
- `K0_BASE`, `K0_NE_SCALE`: 입력 단계 K0 기본값과 NE 반영 계수.
- `LLM_*`: LLM 호출 모델/베이스 URL 환경변수.

## 데이터 구조
- `Vec4`: (D, S, NE, M) 4차원 감정 벡터.
- `DataBox`: 네트워크를 흐르는 패킷(현재 K, NTL, 입력 텍스트(IPT), 전달 경로 trace).
- `Edge`: 뉴런 간 연결. `W`를 통해 가중치를 유지하며, `send()`로 다음 입력 버퍼에 전달한다.

## NTL 추출(LLM 연동)
- `fetch_ntl_from_llm()`는 Ollama API를 호출해 D/S/NE/M JSON을 추출한다.
- 응답이 JSON이 아니면 문자열에서 `{...}` 구간만 파싱하는 폴백 로직을 제공한다.
- `parse_ntl_payload()`는 D/S/NE/M 값을 0~1로 클램프한다.

## Neuron 동작
### 입력 병합
`_combine_inputs()`는 수신한 `DataBox`들의 K/NTL을 가중치(`Edge.W`)로 평균한다. K는 `K_MIN`~`K_MAX` 범위로 클램프된다.

### 발화/전달
- 뉴런은 `K_in`이 임계치(`threshold`)보다 작으면 발화하지 않는다.
- 종류별 출력:
  - `exc`: K 비례로 NTL을 증폭.
  - `inh`: S/D 비율로 NTL을 억제.
  - `reg`: NTL을 그대로 전달.
- 출력 K는 `K_LEAK` 누설과 NE 보너스를 반영한 뒤 `W`를 곱해 재클램프된다.

### 기억 저장
- K가 `K_REMEM` 이상이거나, 뉴런 `W`가 `W_REMEM` 이상일 때 `IPT`를 memory 큐에 저장한다.

### 연결성 변화(ΔW)
- `D, NE`가 높을수록 연결 추가, `S, M`이 높을수록 연결 제거.
- `CONNECTION_DELTA_SCALE`에 의해 변화량 상한/하한을 제한한다.
- 연결 추가는 거리 기반으로 가장 가까운 후보 뉴런을 선택한다.
- 연결 제거는 무작위로 선택하되, 최소 1개는 유지한다.

### 규제(reg) 뉴런의 억제
reg 뉴런은 가까운 몇 개의 대상 뉴런을 선택해 `off_ticks`를 증가시키고, 해당 뉴런이 잠시 입력을 무시하도록 한다.

## Network 동작
- `wire_random(p=0.4)`: 초기 랜덤 연결.
- `tick()`은 각 뉴런의 입력 버퍼 스왑 후 업데이트를 실행한다.
- `inject()`는 입력 뉴런 역할을 하는 더미 노드를 통해 source 뉴런에 `DataBox`를 주입한다.
- `readout()`은 sink 뉴런의 최근 출력 NTL/K와, 주변 뉴런의 메모리를 중복 제거해 반환한다.

## GUI/시각화
- Tkinter 기반 GUI로 입력/LLM 분석/틱 실행을 제공한다.
- Matplotlib 그래프는 NTL, 평균 K, 평균 연결 수(ΔW 의미)를 시간축으로 그린다.

## 로깅
- `logs/`에 실행 시각 기반 로그 파일을 생성한다.
- Inject/LLM 분석/틱 동작을 요약 로그로 남긴다.
