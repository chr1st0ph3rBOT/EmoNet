{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "419b7d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"saved_model\": \"emovec_autorun_cls_out\\\\emovec_autorun_cls.pkl\",\n",
      "  \"report_path\": \"emovec_autorun_cls_out\\\\emovec_autorun_cls_report.json\",\n",
      "  \"report\": {\n",
      "    \"mode\": \"classification->vector_map\",\n",
      "    \"data_used\": \"C:\\\\Users\\\\Admin\\\\Documents\\\\GitHub\\\\emotionAI\\\\감성대화말뭉치(최종데이터)_Training.json\",\n",
      "    \"use_ss\": false,\n",
      "    \"n_train\": 116759,\n",
      "    \"n_test\": 29190,\n",
      "    \"acc\": 0.21531346351490235,\n",
      "    \"f1_macro\": 0.2148529140589591,\n",
      "    \"mae_after_mapping\": 0.09443936279547083,\n",
      "    \"keys\": [\n",
      "      \"dopamine\",\n",
      "      \"serotonin\",\n",
      "      \"norepinephrine\",\n",
      "      \"melatonin\"\n",
      "    ]\n",
      "  },\n",
      "  \"demo_predictions\": [\n",
      "    {\n",
      "      \"text\": \"일이 왜 이렇게 끝이 없지? 화나.\",\n",
      "      \"label\": \"E10\",\n",
      "      \"vector\": [\n",
      "        0.32,\n",
      "        0.28,\n",
      "        0.7,\n",
      "        0.15\n",
      "      ],\n",
      "      \"keys\": [\n",
      "        \"dopamine\",\n",
      "        \"serotonin\",\n",
      "        \"norepinephrine\",\n",
      "        \"melatonin\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[0.32, 0.28, 0.7, 0.15]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "emovec_autorun_cls.py\n",
    "- 인자 없이 실행:  python emovec_autorun_cls.py\n",
    "- DATA_PATH에 있는 JSON/JSONL(샘플과 동일 포맷) 로드\n",
    "- 감정코드(예: \"E18\")를 **분류(LogisticRegression, multinomial, class_weight='balanced')**\n",
    "  로 예측한 뒤, 미리 정의한 EMOTION_VEC 프로토타입으로 4D 벡터로 매핑\n",
    "- 평가: accuracy / macro-F1 (+ 매핑 후 MAE 참고용)\n",
    "- 데모 문장 예측 출력\n",
    "- 모델/리포트 저장\n",
    "\n",
    "사용 전 수정할 부분:\n",
    "    DATA_PATH = \"YOUR_FULL_DATASET.json\"  # 전체 데이터 파일명\n",
    "    OUT_DIR   = \"emovec_autorun_cls_out\"  # 산출물 폴더\n",
    "    USE_SS    = False                     # SS(공감 문장) 포함 여부\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, pathlib, sys\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "\n",
    "# ------------------- SETTINGS -------------------\n",
    "DATA_PATH = \"C:/Users/Admin/Documents/GitHub/emotionAI/감성대화말뭉치(최종데이터)_Training.json\"     # JSON array or JSONL\n",
    "OUT_DIR   = \"emovec_autorun_cls_out\"\n",
    "USE_SS    = False                        # 기본은 HS만 사용(노이즈 감소)\n",
    "\n",
    "TEST_TEXTS = [\n",
    "    \"일이 왜 이렇게 끝이 없지? 화나.\",\n",
    "    \"요즘 회사 생활이 편하고 좋아.\",\n",
    "    \"면접에서 갑자기 예상치 못한 질문이 나와서 당황했어.\",\n",
    "    \"친구들은 다 취업했는데 나만 못 해서 불안해.\",\n",
    "]\n",
    "\n",
    "# ------------------- Emotion code → 4D prototype -------------------\n",
    "EMOTION_VEC = {\n",
    "  \"E10\":[0.32,0.28,0.70,0.15],\n",
    "  \"E11\":[0.55,0.45,0.40,0.35],\n",
    "  \"E12\":[0.45,0.40,0.55,0.25],\n",
    "  \"E15\":[0.28,0.22,0.78,0.12],\n",
    "  \"E16\":[0.55,0.45,0.50,0.30],\n",
    "  \"E18\":[0.30,0.20,0.80,0.10],\n",
    "  \"E19\":[0.35,0.30,0.65,0.18],\n",
    "  \"E20\":[0.25,0.30,0.55,0.20],\n",
    "  \"E21\":[0.40,0.38,0.52,0.28],\n",
    "  \"E22\":[0.22,0.28,0.60,0.18],\n",
    "  \"E23\":[0.48,0.40,0.58,0.30],\n",
    "  \"E24\":[0.42,0.38,0.56,0.26],\n",
    "  \"E25\":[0.33,0.32,0.68,0.18],\n",
    "  \"E26\":[0.30,0.34,0.58,0.22],\n",
    "  \"E30\":[0.38,0.36,0.60,0.24],\n",
    "  \"E32\":[0.34,0.34,0.62,0.22],\n",
    "  \"E33\":[0.45,0.35,0.55,0.25],\n",
    "  \"E35\":[0.40,0.30,0.60,0.20],\n",
    "  \"E36\":[0.44,0.42,0.48,0.30],\n",
    "  \"E37\":[0.50,0.50,0.45,0.35],\n",
    "  \"E39\":[0.46,0.44,0.46,0.34],\n",
    "  \"E40\":[0.28,0.30,0.58,0.22],\n",
    "  \"E42\":[0.36,0.32,0.62,0.22],\n",
    "  \"E44\":[0.30,0.26,0.70,0.16],\n",
    "  \"E47\":[0.35,0.33,0.63,0.20],\n",
    "  \"E49\":[0.26,0.28,0.57,0.20],\n",
    "  \"E50\":[0.40,0.40,0.50,0.30],\n",
    "  \"E51\":[0.52,0.46,0.42,0.32],\n",
    "  \"E52\":[0.44,0.40,0.54,0.28],\n",
    "  \"E53\":[0.40,0.38,0.52,0.30],\n",
    "  \"E54\":[0.36,0.34,0.56,0.28],\n",
    "  \"E55\":[0.34,0.36,0.56,0.26],\n",
    "  \"E56\":[0.48,0.46,0.48,0.32],\n",
    "  \"E57\":[0.42,0.40,0.52,0.30],\n",
    "  \"E58\":[0.40,0.38,0.50,0.30],\n",
    "  \"E59\":[0.43,0.41,0.49,0.31],\n",
    "  \"E60\":[0.62,0.58,0.38,0.42],\n",
    "  \"E62\":[0.35,0.35,0.65,0.20],\n",
    "  \"E64\":[0.70,0.78,0.30,0.48],\n",
    "  \"E65\":[0.76,0.70,0.36,0.44],\n",
    "  \"E66\":[0.60,0.58,0.42,0.38],\n",
    "  \"E67\":[0.68,0.64,0.40,0.42],\n",
    "  \"E68\":[0.80,0.78,0.32,0.46],\n",
    "  \"E69\":[0.82,0.80,0.35,0.46]\n",
    "}\n",
    "EMO_DEFAULT = [0.5, 0.5, 0.5, 0.5]\n",
    "KEYS = [\"dopamine\",\"serotonin\",\"norepinephrine\",\"melatonin\"]\n",
    "\n",
    "# ------------------- Fallback sample (파일 없을 때) -------------------\n",
    "SAMPLE_JSON = [\n",
    "    {\"profile\":{\"emotion\":{\"type\":\"E18\"}},\"talk\":{\"content\":{\"HS01\":\"일은 왜 해도 해도 끝이 없을까? 화가 난다.\", \"SS01\":\"많이 힘드시겠어요.\"}}},\n",
    "    {\"profile\":{\"emotion\":{\"type\":\"E66\"}},\"talk\":{\"content\":{\"HS01\":\"요즘 직장생활이 너무 편하고 좋은 것 같아!\", \"SS01\":\"복지가 좋아서 마음이 편해.\"}}},\n",
    "    {\"profile\":{\"emotion\":{\"type\":\"E35\"}},\"talk\":{\"content\":{\"HS01\":\"면접에서 부모님 직업 질문이 나와서 당혹스러웠어.\", \"SS01\":\"무척 놀라셨겠어요.\"}}},\n",
    "    {\"profile\":{\"emotion\":{\"type\":\"E37\"}},\"talk\":{\"content\":{\"HS01\":\"졸업반이라 취업 걱정은 되지만 너무 불안해하긴 싫어.\", \"SS01\":\"느긋한 태도가 낫다고도 생각해.\"}}},\n",
    "]\n",
    "\n",
    "def load_json_any(path: pathlib.Path):\n",
    "    s = path.read_text(encoding=\"utf-8\").strip()\n",
    "    if s.startswith(\"[\"):\n",
    "        return json.loads(s)\n",
    "    rows = []\n",
    "    for line in s.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def flatten_for_classification(items, min_char=3, use_hs=True, use_ss=False):\n",
    "    X, y = [], []\n",
    "    for ex in items:\n",
    "        try:\n",
    "            emo_type = ex[\"profile\"][\"emotion\"][\"type\"]\n",
    "            content = ex[\"talk\"][\"content\"]\n",
    "            for k, v in content.items():\n",
    "                if not isinstance(v, str):\n",
    "                    continue\n",
    "                s = v.strip()\n",
    "                if not s or len(s) < min_char:\n",
    "                    continue\n",
    "                if k.startswith(\"HS\") and not use_hs:\n",
    "                    continue\n",
    "                if k.startswith(\"SS\") and not use_ss:\n",
    "                    continue\n",
    "                X.append(s); y.append(emo_type)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return X, y\n",
    "\n",
    "def build_model():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=(3,5),\n",
    "            min_df=5,\n",
    "            max_features=300_000,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            multi_class=\"multinomial\",\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=400,\n",
    "            C=2.0,\n",
    "            solver=\"lbfgs\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def map_vec(labels):\n",
    "    return np.asarray([EMOTION_VEC.get(l, EMO_DEFAULT) for l in labels], dtype=float)\n",
    "\n",
    "def IPT2NTL(IPT):\n",
    "    data_path = pathlib.Path(DATA_PATH)\n",
    "    out_dir = pathlib.Path(OUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if data_path.exists():\n",
    "        items = load_json_any(data_path); used_path = str(data_path)\n",
    "    else:\n",
    "        items = SAMPLE_JSON; used_path = \"(embedded SAMPLE_JSON)\"\n",
    "\n",
    "    X, y = flatten_for_classification(items, min_char=3, use_hs=True, use_ss=USE_SS)\n",
    "    if not X:\n",
    "        print(\"No samples parsed. Check DATA_PATH or format.\", file=sys.stderr); sys.exit(2)\n",
    "\n",
    "    # CHANGED: 모델/리포트 경로 미리 정의 + 로드 시도\n",
    "    model_path = out_dir/\"emovec_autorun_cls.pkl\"                 # CHANGED\n",
    "    report_path = out_dir/\"emovec_autorun_cls_report.json\"        # CHANGED\n",
    "    if model_path.exists():                                       # CHANGED\n",
    "        pipe = load(model_path)                                   # CHANGED\n",
    "        # 리포트가 있으면 읽고, 없으면 최소 정보만 채움\n",
    "        try:                                                      # CHANGED\n",
    "            report = json.loads(report_path.read_text(encoding=\"utf-8\"))  # CHANGED\n",
    "        except Exception:                                         # CHANGED\n",
    "            report = {                                            # CHANGED\n",
    "                \"mode\": \"classification->vector_map\",             # CHANGED\n",
    "                \"data_used\": used_path,                           # CHANGED\n",
    "                \"use_ss\": USE_SS,                                 # CHANGED\n",
    "                \"note\": \"loaded pre-trained model\"                # CHANGED\n",
    "            }                                                     # CHANGED\n",
    "    else:\n",
    "        # (기존 학습/평가/저장 블록 유지)\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        pipe = build_model()\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        pred_lbl = pipe.predict(Xte)\n",
    "\n",
    "        acc = accuracy_score(yte, pred_lbl)\n",
    "        f1m = f1_score(yte, pred_lbl, average=\"macro\")\n",
    "        mae_vec = mean_absolute_error(map_vec(yte), map_vec(pred_lbl))\n",
    "\n",
    "        dump(pipe, model_path)  # (기존) 모델 저장\n",
    "        report = {\n",
    "            \"mode\": \"classification->vector_map\",\n",
    "            \"data_used\": used_path,\n",
    "            \"use_ss\": USE_SS,\n",
    "            \"n_train\": len(Xtr), \"n_test\": len(Xte),\n",
    "            \"acc\": float(acc), \"f1_macro\": float(f1m),\n",
    "            \"mae_after_mapping\": float(mae_vec),\n",
    "            \"keys\": KEYS\n",
    "        }\n",
    "        report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding=\"utf-8\")  # (기존) 리포트 저장\n",
    "\n",
    "    # (기존) 데모 예측/결과 구성 — 그대로 둠\n",
    "    demo_lbl = pipe.predict([IPT])\n",
    "    demo_vec = map_vec(demo_lbl).tolist()\n",
    "    demo = [{\"text\": t, \"label\": l, \"vector\": v, \"keys\": KEYS} for t, l, v in zip(TEST_TEXTS, demo_lbl, demo_vec)]\n",
    "\n",
    "    result = [v for v in demo_vec]\n",
    "\n",
    "    print(json.dumps({\n",
    "        \"saved_model\": str(model_path),\n",
    "        \"report_path\": str(report_path),\n",
    "        \"report\": report,\n",
    "        \"demo_predictions\": demo\n",
    "    }, ensure_ascii=False, indent=2))\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(IPT2NTL('일이 왜 이렇게 끝이 없지? 화나.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91b7a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 저장소\n",
    "class emotion_storage():\n",
    "    def __init__(self):\n",
    "        self.NTL_bef = [0] # 이전 상태의 NTL\n",
    "        self.K = []\n",
    "    \n",
    "    def NTL2NTLandK(self, NTL):\n",
    "        K = -70 # 초기 전위\n",
    "        w = 0.1 # 이전 NTL 적용 가중치(10%)\n",
    "        if len(self.K) != 0:\n",
    "            for i, j in enumerate(self.K): # 시간 감쇠를 적용한 K 업데이트\n",
    "                K += i**(1/j)\n",
    "        NTL_bef_w = [i*w for i in self.NTL_bef]\n",
    "        NTL_final = [NTL[i] + NTL_bef_w[i] for i in range(4)]\n",
    "        return NTL_final, K\n",
    "    \n",
    "    def NTL_K_hist(self, NTL, K): # 마지막 모델에서 계산한 NTL, K를 현재 스텝의 최종 NTL, K로 기록\n",
    "        self.NTL_bef = NTL\n",
    "        self.K.append(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1481192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴런 정의\n",
    "K_remember = 100 # 기억 막전위\n",
    "\n",
    "class neuron():\n",
    "    def __init__(self, K_critical, code, type): # 노드 고유 번호(code)\n",
    "        self.K_critical = K_critical # 임계 막전위(활성화를 위한)\n",
    "        self.saved_IPT = [None]\n",
    "        self.code = code\n",
    "        self.connected_node = []\n",
    "        self.connected_node_code = []\n",
    "        self.w_bef = 0\n",
    "        self.w_curr = 0\n",
    "        self.K_out = []\n",
    "        self.NTL_out = []\n",
    "        self.IPT_curr = ''\n",
    "\n",
    "        self.type = type # types: exciting, inhibiting, modulating\n",
    "\n",
    "        self.K_final = 0\n",
    "        self.NTL_final = 0\n",
    "\n",
    "    def do_work(self, NTL, K):\n",
    "        #w 업데이트 시에는 self.w_curr을 업데이트 하기!\n",
    "        # 할일 code needed\n",
    "        return NTL, K\n",
    "    \n",
    "    def save(self, IPT):\n",
    "        self.saved_IPT.append(IPT)\n",
    "        self.IPT_curr = IPT\n",
    "\n",
    "    def make_new_connection(self, node): # 노드와 연결 함수\n",
    "        self.connected_node.append(node)\n",
    "        node.connected_node.append(self)\n",
    "\n",
    "        self.connected_node_code.append(node.code)\n",
    "        node.connected_node_code.append(self.code)\n",
    "        return node.code\n",
    "\n",
    "    def main(self, NTL, K, IPT, node):\n",
    "        if K > self.K_critical:\n",
    "            NTL_out, K_out = self.do_work(NTL, K)\n",
    "            self.NTL_out.append(NTL_out)\n",
    "        if node is not None:\n",
    "            self.K_out.append(K_out * getattr(node, \"w_bef\", 1.0))  # 기본 1.0\n",
    "        else:\n",
    "            self.K_out.append(K_out)\n",
    "\n",
    "        if K > K_remember: # IPT 기억 조건\n",
    "            self.save(IPT)\n",
    "            print('[LOG] IPT Saved')\n",
    "\n",
    "        if self.w_curr - self.w_bef > 0: # 가중치가 커지면 그만크 연결 노드 수 증가\n",
    "            for _ in range(int(self.w_curr - self.w_bef)):\n",
    "                available_node = list(set(node_codes + self.connected_node_code))\n",
    "                if not available_node:\n",
    "                    return  # 또는 continue\n",
    "                self.make_new_connection(nodes[available_node[0]-1])\n",
    "\n",
    "        K_final = sum(K_out) if self.K_out else 0\n",
    "        NTL_final = []\n",
    "        for i in range(4):\n",
    "            k=0\n",
    "            for j in NTL_out:\n",
    "                k += j[i]\n",
    "            NTL_final.append(k/len(NTL_out) if self.K_out/ else 0)\n",
    "\n",
    "        self.NTL_final = NTL_final\n",
    "        self.K_final = K_final\n",
    "        self.update_nodes()\n",
    "        return\n",
    "    \n",
    "    def update_nodes(self):\n",
    "        a_kept = self.saved_IPT.copy()\n",
    "        if self.IPT_curr in a_kept:\n",
    "            idx = a_kept.index(self.IPT_curr)\n",
    "            IPT_out = a_kept[:idx] + a_kept[idx+1:]\n",
    "        else:\n",
    "            IPT_out = [x for x in a_kept if x is not None]\n",
    "\n",
    "        if not IPT_out:\n",
    "            return  # 전파할 IPT가 없으면 종료\n",
    "\n",
    "        for i in self.connected_node:\n",
    "            i.main(self.NTL_final, self.K_final, IPT_out[-1], self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcf3e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn 정의\n",
    "w_exciting = 0.33\n",
    "w_inhibiting = 0.33\n",
    "w_modulating = 0.34\n",
    "node_count = 100\n",
    "\n",
    "# 1) 비율 정리(혹시 합이 1이 아닐 수도 있으니 정규화)\n",
    "weights = [\n",
    "    ('exciting',  w_exciting),\n",
    "    ('inhibiting', w_inhibiting),\n",
    "    ('modulating', w_modulating),\n",
    "]\n",
    "total_w = sum(w for _, w in weights)\n",
    "weights = [(t, (w / total_w) if total_w else 0.0) for t, w in weights]\n",
    "\n",
    "# 2) 바닥 할당 + 나머지를 소수점 큰 순서대로 분배\n",
    "raw = [node_count * w for _, w in weights]\n",
    "base = [int(x) for x in raw]\n",
    "remainder = node_count - sum(base)\n",
    "\n",
    "# 소수 부분 큰 순서대로 remainder 만큼 1씩 추가\n",
    "frac_idx = sorted(range(len(raw)), key=lambda i: (raw[i] - base[i]), reverse=True)\n",
    "for k in range(remainder):\n",
    "    base[frac_idx[k]] += 1\n",
    "\n",
    "# 3) 노드 생성 (code 유니크하게 증가)\n",
    "nodes = []\n",
    "node_codes = []\n",
    "code = 0\n",
    "for (t, _), cnt in zip(weights, base):\n",
    "    for _ in range(cnt):\n",
    "        n = neuron(+30, code, t)   # 네 클래스명 그대로 사용\n",
    "        nodes.append(n)\n",
    "        node_codes.append(code)\n",
    "        code += 1\n",
    "\n",
    "node_count = len(nodes)  # 실제 생성된 수로 갱신\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92812992",
   "metadata": {},
   "source": [
    "**FLOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eca818e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emotion_storage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# class들 호출\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m emotion_storage = \u001b[43memotion_storage\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'emotion_storage' is not defined"
     ]
    }
   ],
   "source": [
    "# class들 호출\n",
    "emotion_storage = emotion_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a97d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 박스 구성\n",
    "data_box = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "data_box['IPT'] = 'I hate you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33785702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 저장소\n",
    "data_box['NTL'] = IPT2NTL(data_box['IPT'])\n",
    "data_box['NTL'], data_box['K'] = emotion_storage.NTL2NTLandK(data_box['NTL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 뉴런 자극\n",
    "import random\n",
    "\n",
    "class dummy_node():\n",
    "    def __init__(self):\n",
    "        self.code = 0\n",
    "\n",
    "dummy_node = dummy_node()\n",
    "\n",
    "start_neuron = nodes[random.randrange(1, node_count+1)+1]\n",
    "start_neuron.main(data_box['NTL'], data_box['K'], data_box['IPT'], dummy_node)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptoss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
